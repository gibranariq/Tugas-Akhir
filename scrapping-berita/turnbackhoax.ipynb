{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URL dari halaman arsip TurnBackHoax\n",
    "# BASE_URL = \"https://turnbackhoax.id/page\"\n",
    "\n",
    "# def scrape_turnbackhoax(start_page, end_page, output_file):\n",
    "#     data = []\n",
    "\n",
    "#     for page in range(start_page, end_page + 1):\n",
    "#         print(f\"Scraping halaman {page}...\")\n",
    "#         response = requests.get(BASE_URL + str(page))\n",
    "#         if response.status_code != 200:\n",
    "#             print(f\"Halaman {page} tidak dapat diakses, melanjutkan ke halaman berikutnya.\")\n",
    "#             continue\n",
    "\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#         # Ambil semua artikel di halaman\n",
    "#         articles = soup.find_all('article', class_='mh-loop-item')\n",
    "        \n",
    "#         for article in articles:\n",
    "#             try:\n",
    "#                 # Judul dan URL\n",
    "#                 title_elem = article.find('h3', class_='entry-title mh-loop-title')\n",
    "#                 title = title_elem.text.strip() if title_elem else \"No Title\"\n",
    "#                 url = title_elem.find('a')['href'] if title_elem and title_elem.find('a') else \"No URL\"\n",
    "\n",
    "#                 # Excerpt (konten singkat dari halaman utama)\n",
    "#                 excerpt_elem = article.find('div', class_='mh-excerpt')\n",
    "#                 excerpt = excerpt_elem.text.strip() if excerpt_elem else \"No Excerpt\"\n",
    "\n",
    "#                 # Mengakses halaman detail untuk mengambil konten lengkap dan tag\n",
    "#                 if url != \"No URL\":\n",
    "#                     detail_response = requests.get(url)\n",
    "#                     detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
    "\n",
    "#                     # Konten lengkap\n",
    "#                     fulltext_elem = detail_soup.find('div', class_='entry-content')\n",
    "#                     fulltext = fulltext_elem.text.strip() if fulltext_elem else \"No FullText\"\n",
    "\n",
    "#                     # Tags\n",
    "#                     tags_section = detail_soup.find('span', class_='tags-links')\n",
    "#                     tags = \", \".join([tag.text for tag in tags_section.find_all('a')]) if tags_section else \"No Tags\"\n",
    "#                 else:\n",
    "#                     fulltext = \"No FullText\"\n",
    "#                     tags = \"No Tags\"\n",
    "\n",
    "#                 # Tambahkan ke data\n",
    "#                 data.append([title, excerpt, fulltext, tags, url])\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error saat memproses artikel: {e}\")\n",
    "\n",
    "#     # Menyimpan data ke file CSV\n",
    "#     with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "#         writer = csv.writer(csv_file)\n",
    "#         writer.writerow([\"Title\", \"Excerpt\", \"FullText\", \"Tags\", \"URL\"])\n",
    "#         writer.writerows(data)\n",
    "\n",
    "#     print(f\"Scraping selesai! Data disimpan di {output_file}\")\n",
    "\n",
    "# # Contoh penggunaan\n",
    "# scrape_turnbackhoax(start_page=1, end_page=2, output_file='turnbackhoax_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped: https://turnbackhoax.id/2024/12/04/salah-kisah-sukses-pemuda-pengangguran-berkat-main-judi-slot/\n",
      "Data berhasil disimpan ke scraped_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk scraping halaman detail berita\n",
    "def scrape_detail_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Mengambil tanggal\n",
    "    date_element = soup.find('span', class_='entry-meta-date updated')\n",
    "    date = date_element.get_text(strip=True) if date_element else \"Date not found\"\n",
    "\n",
    "    # Mengambil judul\n",
    "    title_element = soup.find('h1', class_='entry-title')\n",
    "    title = title_element.get_text(strip=True) if title_element else \"Title not found\"\n",
    "    \n",
    "    # Mengambil isi berita setelah garis pembatas\n",
    "    content = []\n",
    "    separator = soup.find('hr', class_='wp-block-separator has-alpha-channel-opacity')\n",
    "    if separator:\n",
    "        for sibling in separator.find_next_siblings():\n",
    "            # Abaikan headline \"Pemeriksaan Fakta\"\n",
    "            if sibling.name == 'h2' and \"Pemeriksaan Fakta\" in sibling.get_text(strip=True):\n",
    "                continue\n",
    "            \n",
    "            # Tambahkan konten paragraf dan blockquote\n",
    "            if sibling.name == 'blockquote':\n",
    "                content.append(sibling.get_text(strip=True))\n",
    "            elif sibling.name == 'p':\n",
    "                content.append(sibling.get_text(strip=True))\n",
    "            \n",
    "            # Berhenti jika mencapai elemen dengan class kesimpulan atau paragraf terakhir\n",
    "            if sibling.name == 'h2' and \"Kesimpulan\" in sibling.get_text(strip=True):\n",
    "                break\n",
    "\n",
    "    full_text = \"\\n\".join(content).strip()\n",
    "    \n",
    "    return date, title, full_text\n",
    "\n",
    "# URL contoh untuk scraping\n",
    "urls = [\n",
    "    'https://turnbackhoax.id/2024/12/04/salah-kisah-sukses-pemuda-pengangguran-berkat-main-judi-slot/',\n",
    "]\n",
    "\n",
    "# Menyimpan hasil ke file CSV\n",
    "output_file = 'scraped_test.csv'\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['date', 'URL', 'Title', 'Full text']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            date, title, full_text = scrape_detail_page(url)\n",
    "            writer.writerow({'date': date, 'URL': url, 'Title': title, 'Full text': full_text})\n",
    "            print(f\"Scraped: {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "print(f\"Data berhasil disimpan ke {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
