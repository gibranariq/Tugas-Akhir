{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gibran\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# === Import Library ===\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import wordninja\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 494 entries, 0 to 275\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   date       494 non-null    object\n",
      " 1   URL        494 non-null    object\n",
      " 2   Title      494 non-null    object\n",
      " 3   Narrative  494 non-null    object\n",
      " 4   Statement  494 non-null    object\n",
      " 5   Label      494 non-null    int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 27.0+ KB\n",
      "Dataset Information:\n",
      " None\n",
      "Contoh Data:\n",
      "              date                                                URL  \\\n",
      "0    Mei 20, 2019  https://turnbackhoax.id/2019/05/20/benar-klari...   \n",
      "1     Mei 4, 2019  https://turnbackhoax.id/2019/05/04/benar-klari...   \n",
      "2     Mei 3, 2019  https://turnbackhoax.id/2019/05/03/benar-ungga...   \n",
      "3     Mei 2, 2019  https://turnbackhoax.id/2019/05/02/benar-klari...   \n",
      "4  April 30, 2019          https://turnbackhoax.id/2019/04/30/22526/   \n",
      "\n",
      "                                               Title  \\\n",
      "0  [BENAR] Klarifikasi BIN terkait Pesan Berantai...   \n",
      "1  [BENAR] Klarifikasi Putra Aji Adhari, Anak Kel...   \n",
      "2  [BENAR] Unggahan Hasil C1 Pleno TPS di Bangil ...   \n",
      "3  [BENAR] Klarifikasi Surat Edaran Intruksi Mema...   \n",
      "4  [BENAR] Klarifikasi terkait Kabar Petugas KPPS...   \n",
      "\n",
      "                                           Narrative  \\\n",
      "0  Beredar di media sosial sebuah pesan yang beri...   \n",
      "1  Melalu akun instagramnya, Putra Aji Adhari mem...   \n",
      "2  Beredar di media sosial sebuah unggahan formul...   \n",
      "3  Beredar surat edaran yang mengatasnamakan Bada...   \n",
      "4  Kasus penikaman yang menewaskan Sukardi, petug...   \n",
      "\n",
      "                                           Statement  Label  \n",
      "0  29 Tokoh Pendukung Capres 02 Kena Motif. Siapa...      0  \n",
      "1  Anak Kelas ll MTS Bobol Situs KPU Pemilu 2019,...      0  \n",
      "2  22:15  01/05/2019  Hasil C1 PLENO TPS  kenapa ...      0  \n",
      "3  INTRUKSI BADAN PEMENANGAN PRABOWO SANDI\\\\nKepa...      0  \n",
      "4  ~ Sukardi, Simpatisan PKS yang Meninggal Karen...      0  \n"
     ]
    }
   ],
   "source": [
    "# 1. Load Dataset Real dan Fake\n",
    "real_df = pd.read_csv('dataset TA - data real fix.csv')\n",
    "fake_df = pd.read_csv('dataset TA - data fake fix.csv')\n",
    "\n",
    "# 3. Gabungkan Dataset\n",
    "combined_df = pd.concat([real_df, fake_df], axis=0)\n",
    "print(\"Dataset Information:\\n\", combined_df.info())\n",
    "print(\"Contoh Data:\\n\", combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh Data Setelah Preprocessing:\n",
      "              date                                                URL  \\\n",
      "0    Mei 20, 2019  https://turnbackhoax.id/2019/05/20/benar-klari...   \n",
      "1     Mei 4, 2019  https://turnbackhoax.id/2019/05/04/benar-klari...   \n",
      "2     Mei 3, 2019  https://turnbackhoax.id/2019/05/03/benar-ungga...   \n",
      "3     Mei 2, 2019  https://turnbackhoax.id/2019/05/02/benar-klari...   \n",
      "4  April 30, 2019          https://turnbackhoax.id/2019/04/30/22526/   \n",
      "\n",
      "                                               Title  \\\n",
      "0  [BENAR] Klarifikasi BIN terkait Pesan Berantai...   \n",
      "1  [BENAR] Klarifikasi Putra Aji Adhari, Anak Kel...   \n",
      "2  [BENAR] Unggahan Hasil C1 Pleno TPS di Bangil ...   \n",
      "3  [BENAR] Klarifikasi Surat Edaran Intruksi Mema...   \n",
      "4  [BENAR] Klarifikasi terkait Kabar Petugas KPPS...   \n",
      "\n",
      "                                           Narrative  \\\n",
      "0  beredar di media sosial sebuah pesan yang beri...   \n",
      "1  melalu akun instagramnya, putra aji adhari mem...   \n",
      "2  beredar di media sosial sebuah unggahan formul...   \n",
      "3  beredar surat edaran yang mengatasnamakan bada...   \n",
      "4  kasus penikaman yang menewaskan sukardi, petug...   \n",
      "\n",
      "                                           Statement  Label  \n",
      "0  29 tokoh pendukung capres 02 kena motif. siapa...      0  \n",
      "1  anak kelas ll mts bobol situs kpu pemilu 2019,...      0  \n",
      "2  22:15  01/05/2019  hasil c1 pleno tps  kenapa ...      0  \n",
      "3  intruksi badan pemenangan prabowo sandi\\\\nkepa...      0  \n",
      "4  ~ sukardi, simpatisan pks yang meninggal karen...      0  \n"
     ]
    }
   ],
   "source": [
    "# 1. Hapus Emoji\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"    \n",
    "                                u\"\\U0001F600-\\U0001F64F\"  # emoticon\n",
    "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                                u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                                u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                                u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "                                u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# 2. Hapus Karakter Khusus ([], @, dll)\n",
    "def remove_special_chars(text):\n",
    "    # Hapus karakter [ ], @\n",
    "    text = re.sub(r'[\\[\\]@]', '', text)\n",
    "    \n",
    "    # Hapus tanda petik di awal dan akhir teks\n",
    "    text = re.sub(r'(^\"|\"$)', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 3. Hapus URL\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "# 4. Case Folding (Lowercasing)\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# 5. Gabungkan Semua Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_urls(text)\n",
    "    text = lowercase_text(text)\n",
    "    return text\n",
    "\n",
    "# 6. Terapkan Preprocessing\n",
    "combined_df['Narrative'] = combined_df['Narrative'].apply(preprocess_text)\n",
    "combined_df['Statement'] = combined_df['Statement'].apply(preprocess_text)\n",
    "\n",
    "# 7. Simpan Dataset yang Telah Diproses\n",
    "combined_df.to_csv('processed_dataset.csv', index=False)\n",
    "print(\"Contoh Data Setelah Preprocessing:\\n\", combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    combined_df[['Statement', 'Narrative']],  # Hipotesis + Premis\n",
    "    combined_df['Label'],\n",
    "    test_size=0.2,\n",
    "    stratify=combined_df['Label'],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Train dan Test berhasil disimpan:\n",
      "Jumlah Data Train: 395\n",
      "Jumlah Data Test : 99\n"
     ]
    }
   ],
   "source": [
    "# Gabungkan kembali data Train\n",
    "train_df = pd.DataFrame({\n",
    "    'Statement': train_texts['Statement'],\n",
    "    'Narrative': train_texts['Narrative'],\n",
    "    'Label': train_labels\n",
    "})\n",
    "\n",
    "# Gabungkan kembali data Test\n",
    "val_df = pd.DataFrame({\n",
    "    'Statement': val_texts['Statement'],\n",
    "    'Narrative': val_texts['Narrative'],\n",
    "    'Label': val_labels\n",
    "})\n",
    "\n",
    "# Simpan ke CSV\n",
    "train_df.to_csv('train_dataset.csv', index=False)\n",
    "val_df.to_csv('val_dataset.csv', index=False)\n",
    "\n",
    "print(\"Dataset Train dan Test berhasil disimpan:\")\n",
    "print(f\"Jumlah Data Train: {train_df.shape[0]}\")\n",
    "print(f\"Jumlah Data Test : {val_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gibran\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Load IndoBERT Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "# 2. Tokenisasi untuk Simple Model (Statement Saja)\n",
    "train_encodings_simple = tokenizer(\n",
    "    list(train_texts['Statement']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512\n",
    ")\n",
    "val_encodings_simple = tokenizer(\n",
    "    list(val_texts['Statement']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# 3. Tokenisasi untuk NLI Model (Statement + Narrative)\n",
    "train_encodings_nli = tokenizer(\n",
    "    list(train_texts['Statement']),\n",
    "    list(train_texts['Narrative']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512\n",
    ")\n",
    "val_encodings_nli = tokenizer(\n",
    "    list(val_texts['Statement']),\n",
    "    list(val_texts['Narrative']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset_simple = NewsDataset(train_encodings_simple, train_labels)\n",
    "val_dataset_simple = NewsDataset(val_encodings_simple, val_labels)\n",
    "\n",
    "train_dataset_nli = NewsDataset(train_encodings_nli, train_labels)\n",
    "val_dataset_nli = NewsDataset(val_encodings_nli, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Gibran\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 25%|██▌       | 50/200 [23:32<51:27, 20.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17715370655059814, 'eval_runtime': 132.4726, 'eval_samples_per_second': 0.747, 'eval_steps_per_second': 0.098, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 50%|█████     | 100/200 [46:46<34:31, 20.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1694612205028534, 'eval_runtime': 132.0904, 'eval_samples_per_second': 0.749, 'eval_steps_per_second': 0.098, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 150/200 [1:09:33<16:58, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2069169580936432, 'eval_runtime': 132.117, 'eval_samples_per_second': 0.749, 'eval_steps_per_second': 0.098, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 200/200 [1:32:09<00:00, 27.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24072180688381195, 'eval_runtime': 131.2818, 'eval_samples_per_second': 0.754, 'eval_steps_per_second': 0.099, 'epoch': 4.0}\n",
      "{'train_runtime': 5529.4773, 'train_samples_per_second': 0.286, 'train_steps_per_second': 0.036, 'train_loss': 0.161722412109375, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.161722412109375, metrics={'train_runtime': 5529.4773, 'train_samples_per_second': 0.286, 'train_steps_per_second': 0.036, 'train_loss': 0.161722412109375, 'epoch': 4.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"indobenchmark/indobert-base-p1\", num_labels=2\n",
    ")\n",
    "\n",
    "training_args_simple = TrainingArguments(\n",
    "    output_dir=\"./results_simple\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer_simple = Trainer(\n",
    "    model=simple_model,\n",
    "    args=training_args_simple,\n",
    "    train_dataset=train_dataset_simple,\n",
    "    eval_dataset=val_dataset_simple\n",
    ")\n",
    "\n",
    "trainer_simple.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 25%|██▌       | 50/200 [19:28<47:51, 19.14s/it]  \n",
      " 25%|██▌       | 50/200 [21:21<47:51, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17838071286678314, 'eval_runtime': 113.33, 'eval_samples_per_second': 0.874, 'eval_steps_per_second': 0.115, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 100/200 [40:26<31:38, 18.98s/it] \n",
      " 50%|█████     | 100/200 [42:20<31:38, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14524953067302704, 'eval_runtime': 113.7492, 'eval_samples_per_second': 0.87, 'eval_steps_per_second': 0.114, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 150/200 [1:00:14<13:30, 16.20s/it]\n",
      " 75%|███████▌  | 150/200 [1:01:54<13:30, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16194185614585876, 'eval_runtime': 99.8686, 'eval_samples_per_second': 0.991, 'eval_steps_per_second': 0.13, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:18:11<00:00, 18.85s/it]\n",
      "100%|██████████| 200/200 [1:20:08<00:00, 24.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1213579848408699, 'eval_runtime': 117.2055, 'eval_samples_per_second': 0.845, 'eval_steps_per_second': 0.111, 'epoch': 4.0}\n",
      "{'train_runtime': 4808.8796, 'train_samples_per_second': 0.329, 'train_steps_per_second': 0.042, 'train_loss': 0.12649866104125976, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.12649866104125976, metrics={'train_runtime': 4808.8796, 'train_samples_per_second': 0.329, 'train_steps_per_second': 0.042, 'train_loss': 0.12649866104125976, 'epoch': 4.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"indobenchmark/indobert-base-p1\", num_labels=2\n",
    ")\n",
    "\n",
    "training_args_nli = TrainingArguments(\n",
    "    output_dir=\"./results_nli\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer_nli = Trainer(\n",
    "    model=nli_model,\n",
    "    args=training_args_nli,\n",
    "    train_dataset=train_dataset_nli,\n",
    "    eval_dataset=val_dataset_nli\n",
    ")\n",
    "\n",
    "trainer_nli.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:00<00:00,  9.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi Simple Model:\n",
      "\n",
      "Accuracy: 0.93\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "   Real (Entailment)       1.00      0.84      0.91        44\n",
      "Fake (Contradiction)       0.89      1.00      0.94        55\n",
      "\n",
      "            accuracy                           0.93        99\n",
      "           macro avg       0.94      0.92      0.93        99\n",
      "        weighted avg       0.94      0.93      0.93        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:41<00:00,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluasi NLI Model:\n",
      "\n",
      "Accuracy: 0.98\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "   Real (Entailment)       0.98      0.98      0.98        44\n",
      "Fake (Contradiction)       0.98      0.98      0.98        55\n",
      "\n",
      "            accuracy                           0.98        99\n",
      "           macro avg       0.98      0.98      0.98        99\n",
      "        weighted avg       0.98      0.98      0.98        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Evaluasi Simple Model\n",
    "simple_preds = trainer_simple.predict(val_dataset_simple).predictions.argmax(axis=1)\n",
    "simple_accuracy = accuracy_score(val_labels, simple_preds)\n",
    "\n",
    "print(\"Evaluasi Simple Model:\\n\")\n",
    "print(f\"Accuracy: {simple_accuracy:.2f}\")\n",
    "print(classification_report(val_labels, simple_preds, target_names=[\"Real (Entailment)\", \"Fake (Contradiction)\"]))\n",
    "\n",
    "# 2. Evaluasi NLI Model\n",
    "nli_preds = trainer_nli.predict(val_dataset_nli).predictions.argmax(axis=1)\n",
    "nli_accuracy = accuracy_score(val_labels, nli_preds)\n",
    "\n",
    "print(\"\\nEvaluasi NLI Model:\\n\")\n",
    "print(f\"Accuracy: {nli_accuracy:.2f}\")\n",
    "print(classification_report(val_labels, nli_preds, target_names=[\"Real (Entailment)\", \"Fake (Contradiction)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_nli.save_model(\"./first_nli_model_0.98\")\n",
    "trainer_simple.save_model(\"./first_simple_model_0.93\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
